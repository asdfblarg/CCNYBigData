{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f6113a76450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.load('wunderground_2011-2013.csv', format='csv', header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TimeEDT', 'string'),\n",
       " ('TemperatureF', 'string'),\n",
       " ('Dew PointF', 'string'),\n",
       " ('Humidity', 'string'),\n",
       " ('Sea Level PressureIn', 'string'),\n",
       " ('VisibilityMPH', 'string'),\n",
       " ('Wind Direction', 'string'),\n",
       " ('Wind SpeedMPH', 'string'),\n",
       " ('Gust SpeedMPH', 'string'),\n",
       " ('PrecipitationIn', 'string'),\n",
       " ('Events', 'string'),\n",
       " ('Conditions', 'string'),\n",
       " ('WindDirDegrees', 'string'),\n",
       " ('DateUTC', 'string')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5d4572a36cff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wunderground_2011-2017_final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitionsWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_wunder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_2011-2017_fixed.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;31m# df.coalesce(1).write.csv('processed_2011-2017_fixed.csv', header=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dvd/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         \"\"\"\n\u001b[1;32m   1575\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dvd/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m         \"\"\"\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dvd/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/home/dvd/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dvd/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "import csv\n",
    "from dateutil.parser import parse\n",
    "from dateutil.tz import gettz\n",
    "from dateutil import tz\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/New_York')\n",
    "\n",
    "\n",
    "def process_wunder(idx, part):\n",
    "    if idx == 0:\n",
    "        part.next()\n",
    "    from_zone = tz.gettz('UTC')\n",
    "    to_zone = tz.gettz('America/New_York')\n",
    "    for p in csv.reader(part):\n",
    "        # deal with edge case of 'No daily or hourly history data available'\n",
    "        # for 1/28/2012, 1/29/2012, 1/30/2012\n",
    "        if len(p) < 12:\n",
    "            continue\n",
    "        freezing = '0'\n",
    "        windy = '0'\n",
    "        fog = '0'\n",
    "        rain = '0'\n",
    "        snow = '0'\n",
    "        if  float(p[1]) <= 32:\n",
    "            freezing = '1'\n",
    "        try: \n",
    "            if float(p[8]) >= 30:\n",
    "                windy = '1'\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            if  0 <= float(p[5]) <= 2 or 'Fog' in p[10] or 'Mist' in p[11] :\n",
    "                fog = '1'\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        if 'Rain' in p[10] or 'Rain' in p[11] or 'Rain' in p[11] or 'Thunderstorm' in p[11]:\n",
    "            rain = '1'\n",
    "\n",
    "        if 'Snow' in p[10] or 'Snow' in p[11] or 'Snow' in p[10]:\n",
    "            snow = '1'\n",
    "        \n",
    "        DateUTC = parse(p[13])\n",
    "        DatetimeEDT = DateUTC.replace(tzinfo=from_zone).astimezone(to_zone)\n",
    "        \n",
    "        yield Row(  \n",
    "                    DateUTC = p[13],\n",
    "                    TimeEDT = p[0],\n",
    "                    TemperatureF = p[1],\n",
    "                    Dew_PointF = p[2],\n",
    "                    Humidity = p[3],\n",
    "                    Sea_Level_PressureIn = p[4],\n",
    "                    VisibilityMPH = p[5],\n",
    "                    Wind_Direction = p[6],\n",
    "                    Wind_SpeedMPH = p[7],\n",
    "                    Gust_SpeedMPH = p[8],\n",
    "                    PrecipitationIn = p[9],\n",
    "                    Events = p[10],\n",
    "                    Conditions = p[11],\n",
    "                    WindDirDegrees = p[12],\n",
    "                    Freezing = freezing,\n",
    "                    Windy = windy,\n",
    "                    LowVisibility = fog,\n",
    "                    Rain = rain,\n",
    "                    Snow = snow,\n",
    "                    DatetimeEDT = DatetimeEDT,\n",
    "                    )\n",
    "        \n",
    "rows = sc.textFile('wunderground_2011-2017_final.csv').mapPartitionsWithIndex(process_wunder)\n",
    "df = sqlContext.createDataFrame(rows)\n",
    "df.toPandas().to_csv('processed_2011-2017_fixed.csv')\n",
    "# df.coalesce(1).write.csv('processed_2011-2017_fixed.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df.dtypes\n",
    "# df.select(\"DateUTC\", \"Snow\", \"Windy\", \"Rain\").toPandas().to_csv('test2.csv')\n",
    "df.select(\"DateUTC\", \"DatetimeEDT\", \"TimeEDT\").toPandas().to_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "if temp < 32: freezeing bool true\n",
    "wind speed > 31 mph: windy true\n",
    "0< visibilty < 1 mile: low visiiablity\n",
    "rain: rain \n",
    "snow: snow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df.toPandas().to_csv('processed_2011-2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-01-02 06:48:00+00:00\n",
      "2011-01-02 01:48:00-05:00\n"
     ]
    }
   ],
   "source": [
    "from dateutil.parser import parse\n",
    "from dateutil.tz import gettz\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "utc_datetime = '2011-01-02 06:48:00'\n",
    "edt_datetime = '1:48 AM'\n",
    "edt_datetime = '2011-01-02 1:48:00'\n",
    "\n",
    "utc = parse(utc_datetime)\n",
    "\n",
    "from_zone = tz.gettz('UTC')\n",
    "to_zone = tz.gettz('America/New_York')\n",
    "\n",
    "# Tell the datetime object that it's in UTC time zone since \n",
    "# datetime objects are 'naive' by default\n",
    "utc = utc.replace(tzinfo=from_zone)\n",
    "\n",
    "# # Convert time zone\n",
    "est = utc.astimezone(to_zone)\n",
    "\n",
    "print(utc)\n",
    "print(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.take(2)\n",
    "unique_Conditions = df.select(\"Conditions\").distinct()\n",
    "unique_Conditions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|         Conditions|\n",
      "+-------------------+\n",
      "|       Thunderstorm|\n",
      "|Light Freezing Rain|\n",
      "|   Scattered Clouds|\n",
      "|                Fog|\n",
      "|              Clear|\n",
      "|      Partly Cloudy|\n",
      "| Light Freezing Fog|\n",
      "|            Unknown|\n",
      "|      Mostly Cloudy|\n",
      "|         Heavy Rain|\n",
      "|         Light Rain|\n",
      "|               Mist|\n",
      "|               Snow|\n",
      "|               Rain|\n",
      "|               Haze|\n",
      "|           Overcast|\n",
      "|         Light Snow|\n",
      "|         Heavy Snow|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_Conditions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "+------------+\n",
      "|      Events|\n",
      "+------------+\n",
      "|Thunderstorm|\n",
      "|    Fog-Snow|\n",
      "|         Fog|\n",
      "|    Fog-Rain|\n",
      "|        Rain|\n",
      "|        Snow|\n",
      "|            |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_Events = df.select(\"Events\").distinct()\n",
    "print unique_Events.count()\n",
    "unique_Events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('linkId', 'string'),\n",
       " ('linkPoints', 'string'),\n",
       " ('EncodedPolyLine', 'string'),\n",
       " ('EncodedPolyLineLvls', 'string'),\n",
       " ('Transcom_id', 'string'),\n",
       " ('Borough', 'string'),\n",
       " ('linkName', 'string'),\n",
       " ('Owner', 'string')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkinfo = spark.read.load('linkinfo.csv', format='csv', header=True, inferSchema=False)\n",
    "linkinfo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Id', 'string'),\n",
       " ('Speed', 'string'),\n",
       " ('TravelTime', 'string'),\n",
       " ('Status', 'string'),\n",
       " ('DataAsOf', 'string'),\n",
       " ('linkId', 'string')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "april2015 = spark.read.load('april2015.csv', format='csv', header=True, inferSchema=False)\n",
    "april2015.dtypes\n",
    "# april2015.rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4616337', ('1', '16.78', '303', '0', '4/18/2015 09:05:34')),\n",
       " ('4616325', ('2', '21.75', '145', '0', '4/18/2015 09:05:34')),\n",
       " ('4616324', ('3', '16.78', '358', '0', '4/18/2015 09:05:34')),\n",
       " ('4616338', ('4', '21.13', '154', '0', '4/18/2015 09:05:34')),\n",
       " ('4616323', ('106', '16.78', '131', '0', '4/18/2015 09:05:34'))]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def map_linkID(idx, part):\n",
    "    if idx == 0:\n",
    "        part.next()\n",
    "    for p in csv.reader(part):\n",
    "        yield (p[5], (p[0], p[1], p[2], p[3], p[4]) )\n",
    "\n",
    "april2015_rdd = sc.textFile('april2015.csv').mapPartitionsWithIndex(map_linkID)\n",
    "april2015_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4616337',\n",
       "  ('40.74047,-74.009251 40.74137,-74.00893 40.7431706,-74.008591 40.7462304,-74.00797 40.74812,-74.007651 40.748701,-74.007691 40.74971,-74.00819 40.75048,-74.008321 40.751611,-74.00789 40.7537504,-74.00704 40.75721,-74.00463 40.76003,-74.002631 40.7607405,-7',\n",
       "   '}btwFx|ubMsD_AgJcAcR{ByJ_AsBFiEbByCXaFuAkLiDsTaNsPoKmCmB',\n",
       "   'BBBBBBBBBBBBB',\n",
       "   '4616337',\n",
       "   'Manhattan',\n",
       "   '11th ave n ganservoort - 12th ave @ 40th st',\n",
       "   7)),\n",
       " ('4616325',\n",
       "  ('40.73933,-74.01004 40.73895,-74.01012 40.7376,-74.010021 40.7346,-74.01026 40.72912,-74.010781 40.72619,-74.011131',\n",
       "   'y{swFvavbMjANlGSvQn@fa@fBhQdA',\n",
       "   'BBBBBB',\n",
       "   '4616325',\n",
       "   'Manhattan',\n",
       "   '11th ave s ganservoort - west st @ spring st',\n",
       "   7))]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_linkinfo_id(idx, part):\n",
    "    if idx == 0:\n",
    "        part.next()\n",
    "    for p in csv.reader(part):\n",
    "        yield (p[0], (p[1], p[2], p[3], p[4], p[5], p[6],7) )\n",
    "\n",
    "linkinfo_rdd = sc.textFile('linkinfo.csv').mapPartitionsWithIndex(map_linkinfo_id)\n",
    "linkinfo_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4616297',\n",
       "  (('178', '52.2', '162', '0', '4/18/2015 09:05:34'),\n",
       "   ('40.8302204,-73.850641 40.8295305,-73.848401 40.8291004,-73.847491 40.82836,-73.8459 40.8281405,-73.84524 40.8280305,-73.844641 40.827991,-73.843911 40.82804,-73.842981 40.8281906,-73.841761 40.8286304,-73.83844 40.82887,-73.83673 40.8291904,-73.83548 40.8',\n",
       "    '{sexFn}vaMhC_MtAuDrC}Hj@cCTwBFqCIyD]sFwAwSo@uI_AyFe@wBmAmE}A}DwAsCyFqK{DmH}AuD{D_KeCiCiCmAuDQwC`@kH`B',\n",
       "    'BBBBBBBBBBBBBBBBBBBBBBBBB',\n",
       "    '4616297',\n",
       "    'Bronx',\n",
       "    'CBE E CASTLE HILL AVE - BE N WATERBURY AVE',\n",
       "    7))),\n",
       " ('4616297',\n",
       "  (('178', '50.95', '166', '0', '4/18/2015 09:10:34'),\n",
       "   ('40.8302204,-73.850641 40.8295305,-73.848401 40.8291004,-73.847491 40.82836,-73.8459 40.8281405,-73.84524 40.8280305,-73.844641 40.827991,-73.843911 40.82804,-73.842981 40.8281906,-73.841761 40.8286304,-73.83844 40.82887,-73.83673 40.8291904,-73.83548 40.8',\n",
       "    '{sexFn}vaMhC_MtAuDrC}Hj@cCTwBFqCIyD]sFwAwSo@uI_AyFe@wBmAmE}A}DwAsCyFqK{DmH}AuD{D_KeCiCiCmAuDQwC`@kH`B',\n",
       "    'BBBBBBBBBBBBBBBBBBBBBBBBB',\n",
       "    '4616297',\n",
       "    'Bronx',\n",
       "    'CBE E CASTLE HILL AVE - BE N WATERBURY AVE',\n",
       "    7)))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "april2015_rdd.join(linkinfo_rdd).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
