{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fe4c07f6450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data =  sc.textFile('coalesce_joined_weather_collisions_sorted_fixed.csv')\n",
    "df = spark.read.load('coalesce_joined_weather_collisions_sorted_fixed.csv', format='csv', header=True, inferSchema=False)\n",
    "\n",
    "\n",
    "df.registerTempTable('temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(BOROUGH=u'QUEENS', CONTRIBUTING_FACTOR_VEHICLE_1=u'Unspecified', CONTRIBUTING_FACTOR_VEHICLE_2=u'Unspecified', CONTRIBUTING_FACTOR_VEHICLE_3=None, CONTRIBUTING_FACTOR_VEHICLE_4=None, CONTRIBUTING_FACTOR_VEHICLE_5=None, CROSS_STREET_NAME=u'70 DRIVE', DATETIME=u'2016-06-20 21:06:00', LATITUDE=None, LOCATION=None, LONGITUDE=None, NUMBER_OF_CYCLIST_INJURED=u'0', NUMBER_OF_CYCLIST_KILLED=u'0', NUMBER_OF_MOTORIST_INJURED=u'0', NUMBER_OF_MOTORIST_KILLED=u'0', NUMBER_OF_PEDESTRIANS_INJURED=u'0', NUMBER_OF_PEDESTRIANS_KILLED=u'0', NUMBER_OF_PERSONS_INJURED=u'0', NUMBER_OF_PERSONS_KILLED=u'0', OFF_STREET_NAME=None, ON_STREET_NAME=u'METROPOLITAN AVENUE', UNIQUE_KEY=u'3465402', VEHICLE_TYPE_CODE_1=u'PASSENGER VEHICLE', VEHICLE_TYPE_CODE_2=u'PASSENGER VEHICLE', VEHICLE_TYPE_CODE_3=None, VEHICLE_TYPE_CODE_4=None, VEHICLE_TYPE_CODE_5=None, ZIP_CODE=u'11375', _c28=u'58671', Conditions=u'Clear', DateUTC=u'2016-06-21 01:51:00', DatetimeEDT=u'2016-06-20 21:51:00', Dew_PointF=u'63.0', Events=None, Freezing=u'0', Gust_SpeedMPH=u'-', Humidity=u'60', LowVisibility=u'0', PrecipitationIn=u'N/A', Rain=u'0', Sea_Level_PressureIn=u'29.91', Snow=u'0', TemperatureF=u'78.1', TimeEDT=u'9:51 PM', VisibilityMPH=u'9.0', WindDirDegrees=u'0', Wind_Direction=u'Calm', Wind_SpeedMPH=u'Calm', Windy=u'0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.filter(df.LATITUDE != '0')\n",
    "\n",
    "# test = df.filter(df.DATETIME == '2016-06-20 21:06:00')\n",
    "# test = df.filter(if '2016-06-20 21' in df.DATETIME)\n",
    "\n",
    "# test.count()\n",
    "df.first()\n",
    "\n",
    "# a.coalesce(1).write.csv('geodata', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year = \"2016%\"\n",
    "query = 'select * from temp where DATETIME LIKE \"{0}\"'.format(year)\n",
    "temp = sqlContext.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def filter_year(x):\n",
    "    return x.split(\"-\")[0]\n",
    "    \n",
    "year_filter = udf(filter_year, StringType())\n",
    "newcol_df = df.withColumn(\"YEAR\", year_filter(\"DATETIME\"))\n",
    "unique_years = newcol_df.select(\"YEAR\").distinct()\n",
    "unique_years_list = unique_years.rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# unique_years = df.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'2016', u'2017']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_years_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year in unique_years_list:\n",
    "    year_wildcard = year+\"%\"\n",
    "    query = 'select * from temp where DATETIME LIKE \"{0}\"'.format(year_wildcard)\n",
    "    temp = sqlContext.sql(query)\n",
    "#     print(temp.take(1))\n",
    "    temp.coalesce(1).write.csv('partitioned_years/joins_in_'+year, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
